# CryptoCurrency_Forecasting_LSTM


## LSTM
Theoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.

<img width="812" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102358016-e8ee8980-3fa6-11eb-875d-b1890661b33b.png">

At a first sight, this looks intimidating. Let’s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the “memory” of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.

Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.

<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102359579-dffeb780-3fa8-11eb-81cd-5d44b8ac4db2.png">

The way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.

<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102360352-d0cc3980-3fa9-11eb-8af8-77a4b8ef4832.png">
On the LSTM diagram, the top “pipe” is the memory pipe. The input is the old memory (a vector). The first cross ✖ it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.

Then the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the ✖ below the + sign.

After these two operations, you have the old memory C_t-1 changed to the new memory C_t.
<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102360660-35879400-3faa-11eb-9f2a-867c49494ec3.png">

Now lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it’s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.
<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102360812-6667c900-3faa-11eb-8e8e-3b1689487575.png">

Now the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.

The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.

<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102361322-07568400-3fab-11eb-887f-3d52c10dd46c.png">
These two ✖ signs are the forget valve and the new memory valve.
<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102361602-52709700-3fab-11eb-95e0-9bae3d37e0fe.png">

And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.

### Success cases about LSTM
There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.

In 2018, Bill Gates called it a “huge milestone in advancing artificial intelligence” when bots developed by OpenAI were able to beat humans in the game of Dota 2. OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.

In 2018, OpenAI also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.

In 2019, DeepMind's program AlphaStar used a deep LSTM core to excel at the complex video game Starcraft II. This was viewed as significant progress towards Artificial General Intelligence.

## Results of CryptoCurrency Forecasting with LSTM

<img width="800" alt="LSTM_building_Blocks" src="https://user-images.githubusercontent.com/60491493/102361897-a67b7b80-3fab-11eb-9def-a95f219fa4b8.png">
